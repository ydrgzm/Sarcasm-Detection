{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "nawLqhHSW0F7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddb873c-9d7f-4644-d18c-a2dc3b0e026d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 40 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 44.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=b82210630f6dbbc5a0ab785c38449522be6fe64b37ad4cbe801aa8a044db78b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMLm3LrpWtBE",
        "outputId": "2e4e9fbe-224e-473c-b995-18ccea88d981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.811487758945386\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer\n",
        "from pyspark.sql import Row\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('Practice').getOrCreate()\n",
        "\n",
        "# Load and pre-process the training dataset\n",
        "training = spark.read.json(\"/content/drive/MyDrive/Sarcasm_dataset/Sarcasm.json\")\n",
        "training = training.select([\"headline\", \"is_sarcastic\"])\n",
        "training = training.withColumn(\"label\", training[\"is_sarcastic\"].cast(\"int\"))\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "train, test = training.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Define the pipeline for feature extraction and model training\n",
        "tokenizer = Tokenizer(inputCol=\"headline\", outputCol=\"words\")\n",
        "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
        "lr = LogisticRegression(maxIter=10)\n",
        "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
        "\n",
        "# Train the model using the training dataset\n",
        "model = pipeline.fit(train)\n",
        "\n",
        "# Evaluate the model using the test dataset\n",
        "results = model.transform(test)\n",
        "accuracy = results.filter(results.label == results.prediction).count() / results.count()\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Save the model for future use\n",
        "model.write().overwrite().save(\"/content/drive/MyDrive/Sarcasm_dataset/sarcasm_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "\n",
        "# Load the saved pipeline model\n",
        "model = PipelineModel.load(\"/content/drive/MyDrive/Sarcasm_dataset/sarcasm_model\")\n",
        "\n",
        "# Use the model to make predictions on new data\n",
        "predictions = model.transform(test)\n",
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73X6tjBvW2oj",
        "outputId": "22d26a14-8013-414a-8644-92c61010a9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|            headline|is_sarcastic|label|               words|            features|       rawPrediction|         probability|prediction|\n",
            "+--------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|\"how do we allow ...|           0|    0|[\"how, do, we, al...|(262144,[4629,153...|[2.68697919679032...|[0.93625392996719...|       0.0|\n",
            "|#badpicturemonday...|           0|    0|[#badpicturemonda...|(262144,[303,3888...|[6.61101429916212...|[0.99865634109710...|       0.0|\n",
            "|#emojisinthewild ...|           0|    0|[#emojisinthewild...|(262144,[36998,75...|[2.50322494386495...|[0.92436759151578...|       0.0|\n",
            "|#metoo and \"legit...|           0|    0|[#metoo, and, \"le...|(262144,[60775,18...|[4.74087786145573...|[0.99134459194827...|       0.0|\n",
            "|#talktome: lucas ...|           0|    0|[#talktome:, luca...|(262144,[30571,75...|[2.38600631175469...|[0.91575397164491...|       0.0|\n",
            "|'60 minutes' repo...|           0|    0|['60, minutes', r...|(262144,[7484,516...|[1.79657738886261...|[0.85773179197497...|       0.0|\n",
            "|'97 neons to come...|           1|    1|['97, neons, to, ...|(262144,[27576,51...|[-9.8964562207770...|[5.03502702032363...|       1.0|\n",
            "|      'acting white'|           0|    0|   ['acting, white']|(262144,[88110,24...|[4.04887718218498...|[0.98285705851161...|       0.0|\n",
            "|'after earth ii' ...|           1|    1|['after, earth, i...|(262144,[22370,10...|[-4.3252512733796...|[0.01305747197960...|       1.0|\n",
            "|'army of one' cam...|           1|    1|['army, of, one',...|(262144,[16317,27...|[4.6974101750309,...|[0.99096353943843...|       0.0|\n",
            "|'baywatch' offici...|           0|    0|['baywatch', offi...|(262144,[22370,24...|[4.95541966508458...|[0.99300416346237...|       0.0|\n",
            "|'better homes & g...|           1|    1|['better, homes, ...|(262144,[6440,267...|[4.99205620132263...|[0.99325413076614...|       0.0|\n",
            "|'big eyes' is abo...|           0|    0|['big, eyes', is,...|(262144,[18700,54...|[15.8535780707319...|[0.99999986971978...|       0.0|\n",
            "|'black panther' p...|           0|    0|['black, panther'...|(262144,[1628,223...|[8.86808276648512...|[0.99985920752621...|       0.0|\n",
            "|'broad city' crea...|           0|    0|['broad, city', c...|(262144,[1310,275...|[7.01659421503442...|[0.99910392880365...|       0.0|\n",
            "|'but a fox wouldn...|           1|    1|['but, a, fox, wo...|(262144,[1109,218...|[-11.076501558882...|[1.54714057140324...|       1.0|\n",
            "|'candy land' scre...|           1|    1|['candy, land', s...|(262144,[15494,27...|[10.1511174480409...|[0.99996096908289...|       0.0|\n",
            "|'chapter 1: clark...|           1|    1|['chapter, 1:, cl...|(262144,[72030,85...|[-5.3913126450294...|[0.00453532611666...|       1.0|\n",
            "|'dallas' revival ...|           1|    1|['dallas', reviva...|(262144,[16337,27...|[-12.310204636747...|[4.50551145751317...|       1.0|\n",
            "|'deadpool 2' trai...|           0|    0|['deadpool, 2', t...|(262144,[49853,53...|[4.22217405547463...|[0.98554527983671...|       0.0|\n",
            "+--------------------+------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = input(\"Type a sentence: \")\n",
        "\n",
        "df = spark.createDataFrame([(1,sentence)], ['Id','headline'])\n",
        "result = model.transform(df).groupBy(\"prediction\").mean().collect()[0].prediction\n",
        "if result == 1:\n",
        "  print(\"Sarcastic sentence!\")\n",
        "else:\n",
        "  print(\"Normal sentence!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh8TE_Uoiiu2",
        "outputId": "829fb9b0-808d-423c-a2f1-4130a29a42b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a sentence: this alarm clock is great, it never rings.\n",
            "Sarcastic sentence!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the cows went on strike because of hike in milk prices.\n",
        "\n",
        "\n",
        "this alarm clock is great, it never rings.\n",
        "\n"
      ],
      "metadata": {
        "id": "tx9_0c2H_-h7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24IjtLQ3jZkM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}